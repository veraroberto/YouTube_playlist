{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14f673-b9a6-43d7-8005-149bfa1d7d64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from API_KEY import *\n",
    "import pandas as pd\n",
    "from YouTube_fnc import *\n",
    "import isodate, pickle, math, pycountry, requests, string, time, os, itertools, pyperclip, re, unicodedata, time\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo \n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from app import *\n",
    "start_program = time.time()\n",
    "quota_filename = stats_folder / 'Quota.csv'\n",
    "quota = get_today_quota(quota_filename, True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "YT_content_creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4138ea4-697c-4e3f-8f2c-688075e4c567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f54d9-bf6c-49f3-ac1d-5a3488c95bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in restrictions_folder.iterdir():\n",
    "    if file.suffix == '.csv':\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "        except:\n",
    "            print(file)\n",
    "            df = pd.read_csv(file, encoding='cp1252')\n",
    "            write_csv_safely(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92567ee-72ce-4bd8-ad71-b6281c0e401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d7cb1-a4c5-449d-bcc3-da73a8da55fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d22e9-ba4b-4a03-9924-05abce2f6def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf22453-4c37-499d-a79c-4a1e205eb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_videos_to_playlist = choose_option([True, False], 'Add the videos to the Playlists')\n",
    "# add_videos_to_playlist = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb9325-a627-4462-9f48-97b26a3a0170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704cce8-0a9a-4a0d-87f8-e0e48321a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_files = [file for file in playlist_folder.iterdir() if file.suffix == '.txt']\n",
    "playlist_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a51572-9ddd-49d6-b75d-8a2386f9723a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_options = ['Complete DataFrame', \n",
    "                  \"Playlist from a Handles\",\n",
    "                 'Only some Playlist in the iteration',\n",
    "                 'Remove Playlist from the iteration',\n",
    "                  'Only search for Handles']\n",
    "youtube_names_iter = [playlist.stem.replace('_handles', ' ').replace('_', ' ').strip() for playlist in playlist_files]\n",
    "search_handles = choose_option(search_options, 'Search for a Particular Handles')\n",
    "\n",
    "if search_handles == search_options[0]:\n",
    "    YT_content_creators_iter = YT_content_creators\n",
    "elif search_handles == search_options[1]:\n",
    "    YT_content_creators_iter = YT_content_creators[YT_content_creators['uploadsID'].str.startswith('PL')].reset_index(drop=True)\n",
    "elif search_handles == search_options[2]:\n",
    "    clear_output(wait=False)  \n",
    "    playlist_to_search = choose_option(youtube_names_iter, 'Paylist to search new Handles')\n",
    "    file_path = Path('Playlists') / f'{playlist_to_search.replace(\" \", \"_\")}.txt'\n",
    "    handles_filter = file_path.read_text().splitlines()   \n",
    "    youtube_names_iter.remove(playlist_to_search)\n",
    "    while True:\n",
    "        continue_adding = choose_option([True, False], \"Add more Playlist into the :\")\n",
    "        if continue_adding:\n",
    "            clear_output(wait=False) \n",
    "            playlist_to_search = choose_option(youtube_names_iter, 'Paylist to search new Handles')\n",
    "            file_path = Path('Playlists') / f'{playlist_to_search.replace(\" \", \"_\")}.txt'\n",
    "            handles_filter.extend(file_path.read_text().splitlines())\n",
    "            youtube_names_iter.remove(playlist_to_search)\n",
    "        else:\n",
    "            break\n",
    "    YT_content_creators_iter = YT_content_creators[YT_content_creators['Handle'].isin(handles_filter)].reset_index(drop=True)\n",
    "elif search_handles == search_options[3]:\n",
    "    clear_output(wait=False)  \n",
    "    playlist_to_search = choose_option(youtube_names_iter, 'Playlist to remove in the search for new Handles')\n",
    "    file_path = Path('Playlists') / f'{playlist_to_search.replace(\" \", \"_\")}.txt'\n",
    "    handles_filter = file_path.read_text().splitlines()   \n",
    "    youtube_names_iter.remove(playlist_to_search)\n",
    "    while True:\n",
    "        continue_adding = choose_option([True, False], \"Add more Playlist into the :\")\n",
    "        if continue_adding:\n",
    "            clear_output(wait=False) \n",
    "            playlist_to_search = choose_option(youtube_names_iter, 'Paylist to search new Handles')\n",
    "            file_path = Path('Playlists') / f'{playlist_to_search.replace(\" \", \"_\")}.txt'\n",
    "            handles_filter.extend(file_path.read_text().splitlines())\n",
    "            youtube_names_iter.remove(playlist_to_search)\n",
    "        else:\n",
    "            break\n",
    "    YT_content_creators_iter = YT_content_creators[~YT_content_creators['Handle'].isin(handles_filter)].reset_index(drop=True)\n",
    "elif search_handles == search_options[4]:\n",
    "    clear_output(wait=False)\n",
    "    handles_filter = []\n",
    "    while True:\n",
    "        if  choose_option([True, False], \"Add Handles into the new DataFrame:\"):\n",
    "            clear_output(wait=False)\n",
    "            handles_to_search = input('New video in Handle: ').strip().lower()\n",
    "            handles_filter.append(handles_to_search)\n",
    "            \n",
    "            print(f'Handles to be include in the DataFrame: {\", \".join(handles_filter)}')\n",
    "        else:\n",
    "            clear_output(wait=False)\n",
    "            print(f'Handles to be include in the DataFrame: {\", \".join(handles_filter)}')\n",
    "            break\n",
    "    YT_content_creators_iter = YT_content_creators[YT_content_creators['Handle'].isin(handles_filter)].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f801c55-703f-41b9-b823-a5abd44f0150",
   "metadata": {},
   "source": [
    "# **Get all the restircted from a Handle**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "279ba312-af04-4377-aded-aea24870a4e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "start = time.time()\n",
    "Handle = input('Handle: ').strip()\n",
    "if Handle not in YT_content_creators['Handle'].values:\n",
    "    print(f'{Handle} not in the DataFrame ')\n",
    "else:\n",
    "    playlist_id = YT_content_creators[YT_content_creators['Handle'] == Handle]['uploadsID'].values[0]\n",
    "    print(playlist_id)\n",
    "    all_ids_handles = get_all_ids_playlist(youtube, playlist_id, max_iternations = 40)\n",
    "    not_restricted = []\n",
    "    restricted = []\n",
    "    for video_id in all_ids_handles:\n",
    "        response = get_response_video_id(video_id, youtube)\n",
    "        if is_restricted(response):\n",
    "            add_restriction_df(restrictions_folder, Handle, response)\n",
    "            restricted.append(video_id)\n",
    "        else:\n",
    "            not_restricted.append(video_id)\n",
    "    print(f'There are {len(not_restricted)} avilable')\n",
    "    print(f'There are {len(restricted)} restricted')\n",
    "\n",
    "\n",
    "handle_restricted = restrictions_folder / f'{Handle}.csv'\n",
    "print(f'Duration => {duration_string(time.time()-start)}')\n",
    "df = pd.read_csv(handle_restricted)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c1999-190a-4a94-bf37-a8ff9db69044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_firefox_bookmarks(base, partials, output_file=\"bookmarks.html\"):\n",
    "    # Firefox bookmark header\n",
    "    html = [\n",
    "        '<!DOCTYPE NETSCAPE-Bookmark-file-1>',\n",
    "        '<META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=UTF-8\">',\n",
    "        '<TITLE>Bookmarks</TITLE>',\n",
    "        '<H1>Bookmarks</H1>',\n",
    "        '<DL><p>'\n",
    "    ]\n",
    "\n",
    "    for p in partials:\n",
    "        full_url = base.rstrip('/') + p.lstrip('/')\n",
    "        print(full_url)\n",
    "        html.append(f'    <DT><A HREF=\"{full_url}\">{full_url}</A>')\n",
    "\n",
    "    html.append('</DL><p>')\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html))\n",
    "\n",
    "    print(f\"ðŸ”¥ bookmarks created: {output_file}\")\n",
    "# create_firefox_bookmarks(yt_url, df['videoID'].values, output_file=\"Friends 2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8792ea7-6fe1-4e81-b8a7-07b85e8e23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e3752-d311-4530-9574-c338acfa0efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "skip_handle_shorts_path = exceptions_folder / 'skip_shorts_handle.txt'\n",
    "skip_shorts_handle = skip_handle_shorts_path.read_text().splitlines()\n",
    "\n",
    "skip_long_videos_60m_path = exceptions_folder / 'skip_long_videos_60m.txt'\n",
    "skip_long_videos = skip_long_videos_60m_path.read_text().splitlines()\n",
    "\n",
    "skip_live_handle = exceptions_folder /'skip_live_handle.txt'\n",
    "skip_liveStreamingDetails_handle = skip_live_handle.read_text().splitlines()\n",
    "\n",
    "skip_title_path = exceptions_folder /'skip_title.txt'\n",
    "titles_list = skip_title_path.read_text().splitlines()\n",
    "\n",
    "only_add_long_videos_path = Path('Exceptions') / 'only_add_long_videos.txt'\n",
    "only_long_videos = only_add_long_videos_path.read_text().splitlines()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af75db7-3da2-4f1c-a174-4a1fc556a7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RESET = \"\\033[0m\"\n",
    "background = '\\033[48;2;211;211;211m'#   # white/gray background\n",
    "\n",
    "       \n",
    "alignment = YT_content_creators['channelName'].map(len).max()\n",
    "for index, row in YT_content_creators.iterrows():\n",
    "    row_info  = f'{index:03d} {row[\"channelName\"]:<{alignment}} https://www.youtube.com/channel/{row[\"channelID\"]}'\n",
    "    if index % 2 == 0:\n",
    "        print(f\"{background}{row_info} {RESET}\")\n",
    "    else:\n",
    "        print(row_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae413030-ef1c-4ced-8f2b-ac960f29f9da",
   "metadata": {},
   "source": [
    "# **Get the channels not in the DataFrame**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29aae0a7-592c-4c03-aeaf-a214faa203b3",
   "metadata": {},
   "source": [
    "YT_content_creators = pd.read_csv(file_path_yt_creators)\n",
    "initial_quota = get_today_quota(quota_filename, False)\n",
    "subscriptions = get_subscriptions()\n",
    "print(f'Subsribed to {len(subscriptions)} channels')\n",
    "subs_sorted = sorted(subscriptions,  key=lambda x: x[0].lower(), reverse=False)\n",
    "max_len = max(len(str(item[0])) for item in subscriptions)\n",
    "\n",
    "used_quota = get_today_quota(quota_filename, False) - initial_quota\n",
    "print(f'There were {used_quota} quoata used')\n",
    "\n",
    "missing_channel = 1\n",
    "for title, channel_id in subs_sorted:\n",
    "    if not (YT_content_creators['channelID'] == channel_id).any(): \n",
    "        print(f\"{missing_channel:03d} {title:<{max_len}} https://www.youtube.com/channel/{channel_id}\")\n",
    "        missing_channel += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655b64b-17ec-4f1e-b0bf-68cd470c879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e2623-f546-40ee-9b14-68ec0e73e255",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "youtube_names = [playlist.stem.replace('_handles', ' ').replace('_', ' ').strip() for playlist in playlist_files]\n",
    "playlist_dictionary = defaultdict(lambda:\n",
    "                                  {'Handles':[],\n",
    "                                   'Playlist_ID': \"\",\n",
    "                                   \"video_ids\": [],\n",
    "                                   \"new_video_ids\": [],\n",
    "                                    \"new_videos_duration\": 0.0})\n",
    "\n",
    "playlist_names = get_all_playlists(youtube, quota_filename)\n",
    "playlist_names = sorted(playlist_names, key=lambda x: x[\"name\"].lower())\n",
    "shorts_playlist_name = 'Shorts To Watch'\n",
    "other_playlist_name = 'Videos To Watch'\n",
    "Joey_playlist = 'Joey'\n",
    "Jenice_shorts = 'Jenice shorts'\n",
    "TBBT_playlist = 'The Official BBT Podcast'\n",
    "vertical_video_id = 'dHtSz14yQe8'\n",
    "special_playlist = [other_playlist_name, Joey_playlist, TBBT_playlist, Jenice_shorts, shorts_playlist_name]\n",
    "youtube_names.extend(special_playlist)\n",
    "\n",
    "youtube_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505eeaf-a2bf-4cc2-bf49-51b93a04bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482cf794-a8e6-4692-a28e-a7c6fde4bb47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for playlist_name in youtube_names:\n",
    "    #Getting the Playlist ID\n",
    "    playlist_id = next((d[\"id\"] for d in playlist_names if playlist_name in d.values()), \"\")\n",
    "    playlist_dictionary[playlist_name]['Playlist_ID'] = playlist_id\n",
    "    #Getting the Handles of the Playlist\n",
    "    playlist_file = playlist_name.replace(' ', '_') + '.txt'\n",
    "    file_path = playlist_folder / playlist_file\n",
    "    if file_path.exists():\n",
    "        handles = get_elements_from_file(file_path)\n",
    "        playlist_dictionary[playlist_name]['Handles'].extend(handles)\n",
    "    #Getting the Current videos IDs of the Playlist\n",
    "    if playlist_id:\n",
    "        video_ids = get_all_ids_playlist(youtube, quota_filename, playlist_id, 20)\n",
    "        playlist_dictionary[playlist_name]['video_ids'].extend(video_ids)\n",
    "print(f'Duration {duration_string(time.time() - start)} ')\n",
    "video_ids_in_any_playlist = [vid_id for handle in playlist_dictionary for vid_id in playlist_dictionary[handle]['video_ids']]\n",
    "print(f'There are {len(video_ids_in_any_playlist)} videos ids in all the playlists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25734997-11bd-4d9c-9df1-8ac7f517373e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9c43e-fed9-43c9-a746-560e5a8d2b08",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "ids_not_in_files = []\n",
    "missing_ids_to_sort = []\n",
    "inicio = time.time()\n",
    "checked_ids = []\n",
    "missing_handle = set()\n",
    "all_strings = load_all_strings_txt_folder(Content_Creator_folder)\n",
    "for playlist_item in playlist_dictionary:\n",
    "    for video_index, video_id in enumerate(playlist_dictionary[playlist_item]['video_ids'], 1):\n",
    "        if video_id not in all_strings:\n",
    "            response = get_response_video_id(youtube, quota_filename, video_id)\n",
    "            items =  response.get('items', [])\n",
    "            if not items:\n",
    "                continue\n",
    "            publishedAt = items[0]['snippet']['publishedAt']\n",
    "            metadata_video = get_metadata_video(response)          \n",
    "            checked_ids.append(metadata_video)\n",
    "            if metadata_video == ():\n",
    "                separator = '+-'*50\n",
    "                print(separator)\n",
    "                print(f'Playlist: {playlist_item}')\n",
    "                print(f'\\t The video index {video_index} was deleted')\n",
    "                print('\\t' + yt_url + video_id)\n",
    "                print('-'*90)\n",
    "                print(separator)\n",
    "                playlist_id_w_video_id = playlist_dictionary[playlist_item]['playlist_id']\n",
    "                delelte_video_id_from_playlist(youtube, video_id, playlist_id_w_video_id)\n",
    "\n",
    "            elif response['items']:\n",
    "                metadata_video.insert(0,f'Playlist: {playlist_item}')\n",
    "                channelId = items[0]['snippet']['channelId']\n",
    "                if channelId in YT_content_creators['channelID'].values and not YT_content_creators[YT_content_creators['channelID'] == channelId]['uploadsID'].iloc[0].startswith('PL'):\n",
    "                    handle = YT_content_creators[YT_content_creators['channelID'] == channelId]['Handle'].iloc[0]\n",
    "                    file_path = Content_Creator_folder / f'{handle}.txt'\n",
    "                    info_dictionary = {'file_path': file_path, 'video_id': video_id, 'publishedAt':publishedAt}\n",
    "                    ids_not_in_files.append(info_dictionary)\n",
    "                else:\n",
    "                    # handle = get_podcast_name_videoID(YT_content_creators, video_id, channelId)\n",
    "                    handle = get_podcast_name_videoID(youtube, quota_filename, YT_content_creators, video_id, channelId)\n",
    "                    if handle:\n",
    "                        file_path = Content_Creator_folder / f'{handle}.txt'\n",
    "                        info_dictionary = {'file_path': file_path, 'video_id': video_id, 'publishedAt':publishedAt}\n",
    "                        ids_not_in_files.append(info_dictionary)\n",
    "                    ################################################\n",
    "                    # Search for  \n",
    "                    else:\n",
    "                        missing_ids_to_sort.append(video_id)\n",
    "                        response_channel = get_channel_response(youtube, quota_filename, channelId)\n",
    "                        handle_to_sort = response_channel['items'][0]['snippet']['customUrl']\n",
    "                        missing_handle.add(handle_to_sort)\n",
    "\n",
    "print(f'The duration of the process was {duration_string(time.time() - inicio)}')\n",
    "get_today_quota(quota_filename, True)\n",
    "if ids_not_in_files: \n",
    "    num_videos = len(ids_not_in_files)\n",
    "    print(f'There were {num_videos} videos added manually and saved {num_videos*50} quota')\n",
    "    ids_not_in_files.sort(key=lambda x: datetime.strptime(x['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "    for video_info in ids_not_in_files:\n",
    "        file_path = video_info['file_path']\n",
    "        video_id = video_info['video_id']\n",
    "        add_element_to_file(file_path, video_id, False)\n",
    "else:\n",
    "    print(\"There wasn't any video added manually to any Playlist\")\n",
    "\n",
    "if len(missing_ids_to_sort) > 0:   \n",
    "    print(f'There are {len(missing_ids_to_sort)} Handles to sort')\n",
    "    for i, vid in enumerate(missing_ids_to_sort,1):\n",
    "        print(f'{i:02d} {yt_url + vid}')\n",
    "if missing_handle:\n",
    "    print('Handles to Sort')\n",
    "for h in missing_handle:\n",
    "    print('\\t' + 'https://www.youtube.com/' + h)\n",
    "\n",
    "if checked_ids:\n",
    "    print('The videos ids added manually')\n",
    "    for video_info in checked_ids:\n",
    "        for info in video_info:\n",
    "            print('\\t' +  info)\n",
    "        print('*'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2eb3f-558d-4c23-bdfc-ef850cd5f57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e62082-9e1e-44e6-ac10-88acf3aa037c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "inicio = time.time()\n",
    "restricted_videos = []\n",
    "\n",
    "num_rows = len(YT_content_creators_iter)\n",
    "check_for_error = []\n",
    "upcoming_video_ids = []\n",
    "shorts_skiped = []\n",
    "livestreams = []\n",
    "exceptions = []\n",
    "total_duration = 0\n",
    "ids_with_error = []\n",
    "was_breaked = False\n",
    "check_csv = []\n",
    "\n",
    "for index, row in YT_content_creators_iter.iterrows():\n",
    "    if was_breaked:\n",
    "        break\n",
    "    start = time.time()\n",
    "    channel_id = row['channelID']\n",
    "    Handle = row['Handle']\n",
    "    unploads_ID = row['uploadsID']\n",
    "    Channel_Name = row['channelName']\n",
    "    clear_output(wait=True)\n",
    "    print(f'{index + 1:03d} / {num_rows}: {Channel_Name}', end='\\r')\n",
    "    video_ids_yt = get_all_ids_playlist(youtube, quota_filename, unploads_ID, 1)\n",
    "    if not video_ids_yt:\n",
    "        check_for_error.append(Handle)\n",
    "    video_ids_yt.reverse()\n",
    "    file_path = Content_Creator_folder / f'{Handle}.txt'\n",
    "    downloaded_video = get_elements_from_file(file_path)\n",
    "    key = next(\n",
    "        (key for key, handles in playlist_dictionary.items() if Handle in handles.get('Handles',[])),\n",
    "        None)\n",
    "    for video_id in video_ids_yt:\n",
    "        if video_id not in downloaded_video:\n",
    "            if video_id in video_ids_in_any_playlist and Handle.lower() != 'bigbangtheory':\n",
    "                continue               \n",
    "            else:\n",
    "                response = get_response_video_id(youtube, quota_filename, video_id)\n",
    "                items = response.get('items',[])\n",
    "                if not items:\n",
    "                    add_element_to_file(file_path,video_id, False)\n",
    "                    ids_with_error.append((Handle, yt_url + video_id))\n",
    "                    continue\n",
    "                contentDetails = items[0].get('contentDetails', {})\n",
    "                snippet = items[0].get('snippet', {})\n",
    "                #Timestamp of the first YouTube video ever published \n",
    "                publishedAt = snippet.get('publishedAt', \"2005-04-24T03:31:52Z\") \n",
    "                title = snippet.get('title', \"\")\n",
    "                channelTitle = snippet.get('channelTitle',\"\")\n",
    "                duration_iso = contentDetails.get('duration', 'PT0S')\n",
    "                duration = isodate.parse_duration(duration_iso).total_seconds()\n",
    "                video_id_info = {'video_id': video_id, 'publishedAt': publishedAt, \n",
    "                    'title': title,'file_path': file_path,'duration' : duration}\n",
    "                liveBroadcastContent = snippet.get('liveBroadcastContent', None)\n",
    "                liveStreamingDetails = items[0].get('liveStreamingDetails', None)\n",
    "                if liveBroadcastContent == 'upcoming' or duration == 0:\n",
    "                    upcoming_video_ids.append(video_id)\n",
    "                    continue\n",
    "                elif is_restricted(response):\n",
    "                    add_element_to_file(file_path,video_id, False)\n",
    "                    restriction = contentDetails.get('regionRestriction',{})\n",
    "                    restricted_videos.append([Handle, title, channelTitle, publishedAt, restriction, yt_url + video_id])\n",
    "                    try:\n",
    "                        add_restriction_df(restrictions_folder, Handle, response)\n",
    "                    except:\n",
    "                        check_csv.append(Handle)\n",
    "                    continue\n",
    "                elif liveStreamingDetails and Handle in skip_liveStreamingDetails_handle:\n",
    "                    add_element_to_file(file_path,video_id, False)\n",
    "                    livestreams.append(video_id)\n",
    "                    continue\n",
    "                elif (Handle in only_long_videos and duration < 35*60) or \\\n",
    "                any(remove_accents(t.lower()) in remove_accents(title.lower()) for t in titles_list) or \\\n",
    "                (Handle in skip_long_videos and duration >= 60*60) or duration >= 60*60*3:\n",
    "                    add_element_to_file(file_path, video_id, False)\n",
    "                    exceptions.append(video_id)\n",
    "                    continue   \n",
    "                elif Handle.lower() == 'friends' and 'FULL EPISODE' in title:\n",
    "                    special_playlist_dictionary[Joey_playlist]['new_video_ids'].append(video_id_info)\n",
    "                elif Handle.lower() == 'bigbangtheory' and 'The Official BBT Podcast' in title:\n",
    "                    special_playlist_dictionary[TBBT_playlist][new_videos_id].append(video_id_info)\n",
    "                    playlist_dictionary[key][new_videos_id].append(video_id_info)\n",
    "                else:\n",
    "                    short = is_short(video_id)\n",
    "                    if short == 'Error':\n",
    "                        was_breaked = True\n",
    "                        break\n",
    "                    elif short == 'Is short':                 \n",
    "                        if Handle == 'jeniceofficial':\n",
    "                            playlist_dictionary[Jenice_shorts]['new_video_ids'].append(video_id_info)\n",
    "                            playlist_dictionary[Jenice_shorts]['new_videos_duration'] += duration\n",
    "                        elif Handle not in skip_shorts_handle:\n",
    "                            playlist_dictionary[shorts_playlist_name]['new_video_ids'].append(video_id_info)\n",
    "                            playlist_dictionary[shorts_playlist_name]['new_videos_duration'] += duration\n",
    "                        else:\n",
    "                            add_element_to_file(file_path, video_id, False)\n",
    "                            shorts_skiped.append(video_id)\n",
    "                    elif key:\n",
    "                        playlist_dictionary[key]['new_video_ids'].append(video_id_info)\n",
    "                        playlist_dictionary[key]['new_videos_duration'] += duration\n",
    "                    else:\n",
    "                        playlist_dictionary[other_playlist_name]['new_video_ids'].append(video_id_info)\n",
    "                        playlist_dictionary[other_playlist_name]['new_videos_duration'] += duration\n",
    "                total_duration += duration\n",
    "        if was_breaked:\n",
    "            break\n",
    "if not was_breaked:\n",
    "    clear_output(wait=False)  \n",
    "    current_quota = get_today_quota(quota_filename, True)\n",
    "    print(f'In total, the duration of the new videos is => {duration_string(total_duration)} ')\n",
    "    print(f'The duration of the process was {duration_string(time.time() - inicio)}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484e7fe-e1f5-4b4c-a4ff-d491c27930d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_short(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee36342-c4e5-4717-b2d8-7cabc9d82359",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbfe66-c2c3-49fb-b24b-626fece159fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_quota = get_today_quota(quota_filename, True)\n",
    "if ids_with_error:\n",
    "    separator_string = '/'*100\n",
    "    print(separator_string)\n",
    "    print('Videos ID without items:')\n",
    "    for video_info in ids_with_error:\n",
    "        for i in video_info:\n",
    "            print(i)\n",
    "        print('!'*40)\n",
    "    print(separator_string)\n",
    "    \n",
    "if restricted_videos:\n",
    "    separator_string = '*'*100\n",
    "    print(separator_string)\n",
    "    print('Restricted Videos:')\n",
    "    for video_info in restricted_videos:\n",
    "        for info in video_info:\n",
    "            if isinstance(info, dict):\n",
    "                 for key, values in info.items():\n",
    "                    print(f\"\\t{key}: \", end=\"\")\n",
    "                    print(\", \".join(values))\n",
    "            else:\n",
    "                print('\\t' + info)\n",
    "        print('\\t'+'+' * 92)\n",
    "    print(separator_string)\n",
    "if shorts_skiped:\n",
    "    separator_string = 'x'*100\n",
    "    print(separator_string)\n",
    "    print('Skipped shorts: ')\n",
    "    for index, short in enumerate(shorts_skiped,1):\n",
    "        print(f'\\t{index:02d}: {yt_url}{short}')\n",
    "    print(separator_string)\n",
    "if livestreams:\n",
    "    separator_string = '+'*100\n",
    "    print(separator_string)\n",
    "    print('Livesteams not added to any playlist:')\n",
    "    for index, live in enumerate(livestreams, 1):\n",
    "        print(f'{index:02d}: {yt_url}{live}')\n",
    "    print(separator_string)\n",
    "if exceptions:\n",
    "    separator_string = '+'*100\n",
    "    print(separator_string)\n",
    "    print('Video not added to any playlist by exception, time or title:')\n",
    "    for index, exception_id in enumerate(exceptions, 1):\n",
    "        print(f'\\t{index:02d}: {yt_url}{exception_id}')\n",
    "    print(separator_string)\n",
    "if upcoming_video_ids:\n",
    "    separator_string = '-'*100\n",
    "    print(separator_string)\n",
    "    print('Future videos:')\n",
    "    for vid in upcoming_video_ids:\n",
    "        print(f'\\t{yt_url}{vid}')\n",
    "    print(separator_string)\n",
    "if check_for_error:\n",
    "    print('Handles with error')\n",
    "    for h in check_for_error:\n",
    "        print('\\t' + h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73f24c-db64-4114-a9fa-1ef9fc15a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_sorted = list(playlist_dictionary.keys())\n",
    "keys_sorted\n",
    "keys_sorted.append(keys_sorted.pop(keys_sorted.index(Jenice_shorts)))\n",
    "keys_sorted.append(keys_sorted.pop(keys_sorted.index(shorts_playlist_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b572f08-5cfd-4394-a271-0f58a6bfbe5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_vids_to_add = 0\n",
    "\n",
    "for i, yt_playlist in enumerate(keys_sorted):\n",
    "    new_video_ids =  playlist_dictionary[yt_playlist].get('new_video_ids', [])\n",
    "    if new_video_ids:\n",
    "        print(yt_playlist)\n",
    "        to_add = len(new_video_ids)\n",
    "        total_vids_to_add += to_add\n",
    "        for index, new_id in enumerate(new_video_ids, 1):\n",
    "            short = is_short(new_id[\"video_id\"])\n",
    "            if short is True:\n",
    "                \n",
    "                print(f'\\t{index:03d} {yt_url}{new_id[\"video_id\"]}')\n",
    "            \n",
    "        \n",
    "    \n",
    "            \n",
    "\n",
    "        print('\\t' + '*'*55)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6095e5-4b27-4ca7-bce7-8cb265579f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "24d93338-0ce0-48ae-817f-577e05a053b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "if not add_videos_to_playlist:\n",
    "    new_keys = []\n",
    "    if choose_option([True, False], 'Add only some Keys:'):\n",
    "        add_videos_to_playlist = True\n",
    "        while True:\n",
    "            if new_keys:\n",
    "                print(f'{\", \".join(new_keys)}')\n",
    "            n_k = choose_option(keys_sorted, 'Add only Some Keys:')\n",
    "            if n_k == None:\n",
    "                break\n",
    "            keys_sorted.pop(keys_sorted.index(n_k))\n",
    "            new_keys.append(n_k)\n",
    "            clear_output(wait=False)\n",
    "            if not choose_option([True, False], 'Add more keys:'):\n",
    "                    break\n",
    "        keys_sorted.clear()\n",
    "        keys_sorted.extend(new_keys)\n",
    "        if Jenice_shorts in keys_sorted:\n",
    "            keys_sorted.append(keys_sorted.pop(keys_sorted.index(Jenice_shorts)))\n",
    "        if shorts_playlist_name in keys_sorted:\n",
    "            keys_sorted.append(keys_sorted.pop(keys_sorted.index(shorts_playlist_name)))\n",
    "        print(keys_sorted)\n",
    "        total_vids_to_add = 0\n",
    "        for i, yt_playlist in enumerate(keys_sorted):\n",
    "            new_video_ids =  playlist_dictionary[yt_playlist].get('new_video_ids', [])\n",
    "            if new_video_ids:\n",
    "                print(yt_playlist)\n",
    "                to_add = len(new_video_ids)\n",
    "                total_vids_to_add += to_add\n",
    "                for index, new_id in enumerate(new_video_ids, 1):\n",
    "                    print(f'\\t{index:02d} {yt_url}{new_id[\"video_id\"]}')\n",
    "                print('\\t' + '*'*55)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecffd5-8cf5-4505-b637-d874370fa278",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_quota = current_quota + total_vids_to_add * 50\n",
    "print(f'There are {total_vids_to_add} total videos to add')\n",
    "print(f'There are going to consume {total_vids_to_add * 50:,} quoata usage')\n",
    "print(f'The final quota is going to be {last_quota:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c5728-0292-437a-aa4b-e70c39517d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "added_videos = defaultdict(lambda:\n",
    "                          {'video_info': [],\n",
    "                          'duration' : 0.0})\n",
    "continue_adding_playlistitems = True\n",
    "initial_quota = get_today_quota(quota_filename, False)\n",
    "impossible_to_create = []\n",
    "if add_videos_to_playlist:\n",
    "    quota_limit = 7000\n",
    "    missing_vido_ids = defaultdict(list)\n",
    "    clear_print = True\n",
    "    inicio = time.time()\n",
    "    for index, yt_playlist in enumerate(keys_sorted, 1):\n",
    "        new_ids = playlist_dictionary[yt_playlist]['new_video_ids']\n",
    "        new_ids.sort(key=lambda x: datetime.strptime(x['publishedAt'], \"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "        if new_ids:\n",
    "            print(f'Adding the videos to: {yt_playlist}')\n",
    "\n",
    "        playlist_id = playlist_dictionary[yt_playlist]['Playlist_ID']\n",
    "        digits = len(str(len(new_ids)))\n",
    "        for i, video_info in enumerate(new_ids, 1):\n",
    "            #Video info is a dictionary with the keys # 'video_id', 'publishedAt', 'title', 'file_path', duration\n",
    "            print(f'{i:0{digits}d} / {len(new_ids)}', end='\\r')\n",
    "            if  get_today_quota(quota_filename, False) >= quota_limit:\n",
    "                missing_vido_ids[yt_playlist].append(video_info['video_id'])\n",
    "                continue_adding_playlistitems = False\n",
    "\n",
    "            elif video_info['video_id'] not in video_ids_in_any_playlist:\n",
    "                if not playlist_id:\n",
    "                    response_playlist = create_private_playlist(youtube, quota_filename, yt_playlist, yt_playlist)\n",
    "                    time.sleep(1)\n",
    "                    playlist_id = response_playlist.get('id', \"\")\n",
    "                    if not response_playlist or not playlist_id:\n",
    "                        impossible_to_create.append(yt_playlist)\n",
    "                        break\n",
    "                    playlist_dictionary[yt_playlist]['Playlist_ID'] = playlist_id\n",
    "                    clear_print = False\n",
    "                    if yt_playlist in [Jenice_shorts, shorts_playlist_name]:\n",
    "                        add_video_to_playlist(youtube, playlist_id, vertical_video_id)\n",
    "                        \n",
    "                if add_video_to_playlist(youtube, quota_filename, playlist_id, video_info['video_id']):\n",
    "                    add_element_to_file(video_info['file_path'],video_info['video_id'], False)\n",
    "                    added_videos[yt_playlist]['video_info'].append(video_info)\n",
    "                    added_videos[yt_playlist]['duration'] += video_info['duration']\n",
    "                    if 'The Official BBT Podcast' not in video_info['title']:\n",
    "                        video_ids_in_any_playlist.append(video_info['video_id'])\n",
    "        if clear_print:\n",
    "            clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48629bce-cecc-490d-b344-18e6b7c8f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_keys = list(added_videos.keys())\n",
    "if added_videos:\n",
    "    alignment = max(len(key) for key in added_videos)\n",
    "#     val_alignment = max(len(str(value[\"num_videos\"])) for value in added_videos.values())\n",
    "    val_alignment = 2\n",
    "    ansi_pattern = re.compile(r'\\x1b\\[[0-9;]*m')  \n",
    "    keys_added_sort = sorted(added_videos.items(), key=lambda x: x[1][\"duration\"], reverse=True)\n",
    "    for key in keys_added_sort:\n",
    "        bold_key = f\"\\033[1;4m{key[0]}:\\033[0m\" \n",
    "        extra_alignmnet = len(bold_key) - len(ansi_pattern.sub('', bold_key)) + 1\n",
    "        print(f'{bold_key:<{alignment + extra_alignmnet}}   {len(key[1][\"video_info\"]):>{val_alignment}}  => {duration_string(key[1][\"duration\"]):>{val_alignment}}')  \n",
    "\n",
    "final_quota = get_today_quota(quota_filename, True)\n",
    "print(f'There were used {(final_quota - initial_quota):,} quoatas')\n",
    "print(f'The duration of the process was {duration_string(time.time() - inicio)}')\n",
    "print(f'There are {sum(len(playlist[\"video_info\"]) for playlist in added_videos.values())} new videos added in all the playlist')\n",
    "print(f'The duration of the whole program was {duration_string(time.time() - start_program)}')\n",
    "if impossible_to_create:\n",
    "    print(f'Impossible to create {\", \".join(impossible_to_create)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d61b1-0985-4b62-8460-4ec094ad02b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "969e8f5c-1d42-4f64-ac90-271ce01369c2",
   "metadata": {},
   "source": [
    "for key in missing_vido_ids.items():\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a87d1-1a91-4e0f-a3c0-7828a5e1e672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videos_to_add_missing = 0\n",
    "if not continue_adding_playlistitems:\n",
    "    print(f'There are {sum(len(key) for key in missing_vido_ids.values())} missing videos')\n",
    "    print('*'*50)\n",
    "    for key in missing_vido_ids.keys():\n",
    "        print(key)\n",
    "        for i, video_id in enumerate(missing_vido_ids[key], 1):\n",
    "            print(f'\\t{i:02d} {yt_url}{video_id}')\n",
    "            videos_to_add_missing += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb40be-ef84-45be-8dd9-4627ba397067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4e777ae5-0700-41cb-aefb-b86a90e49429",
   "metadata": {},
   "source": [
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "data = {\n",
    "    \"Apples\": 23,\n",
    "    \"Bananas\": 1107,\n",
    "    \"Oranges\": 8,\n",
    "    \"Watermelons\": 56\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "table = Table(show_header=False, box=False)\n",
    "table.add_column(\"Item\", justify=\"left\")r\n",
    "table.add_column(\"Count\", justify=\"right\")\n",
    "for key, value in data.items():\n",
    "    table.add_row(f\"[bold underline]{key}:\", str(value))\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "736f285e-5fe5-414e-a3b1-adcb7cc97d82",
   "metadata": {},
   "source": [
    "table = Table(show_header=False, box=False)\n",
    "table.add_column(\"Item\", justify=\"left\")\n",
    "table.add_column(\"Count\", justify=\"right\")\n",
    "for key in added_keys:\n",
    "    table.add_row(f\"[bold underline]{key}:\", str(added_videos[key]))\n",
    "console.print(table)\n",
    "print('Hola')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bbcf98e-5b21-48b0-b86d-b416a6b1949e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caed32d1-6eb3-496f-a9bc-26015245057b",
   "metadata": {},
   "source": [
    "\n",
    "df = pd.read_csv(quota_filename)\n",
    "\n",
    "# Make sure the first column is parsed as dates\n",
    "df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df.iloc[:, 0], df.iloc[:, 1], marker='o')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Quota')\n",
    "plt.title('Values over Time')\n",
    "\n",
    "# Improve readability of dates\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391359ad-69ab-4fd6-9ddf-2209948f0fb2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "playlist_dictionary['Shorts To Watch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270e2bc-4293-4bf3-88e5-a92c7cf32160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
